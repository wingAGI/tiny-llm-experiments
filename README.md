# tiny-llm-experiments

This project is dedicated to experimenting with large language models (LLMs), focusing on exploring and validating the characteristics of different components within these models. 

## Project Structure
- `word_and_positional_embeddings.py`: A script to calculate and analyze whether word embeddings and positional encoding vectors reside in their respective subspaces.
- Additional experiment scripts and data files (to be added later).

## Experiments

### 1. Word Embeddings and Positional Encoding Subspace Analysis

This experiment aims to verify whether the word embeddings and positional encoding vectors in language models truly exist in separate subspaces. 

For more details, you can refer to the specific [note on xiaohongshu](https://www.xiaohongshu.com/explore/6807c26e000000001d001dd4?xsec_token=ABXcl56bpqQYwofcEBb5-U8THF3uft0f-CjbDygKEPrMw=&xsec_source=pc_user)ï¼Œwhere we have documented the process and results in detail

#### How to Run
1. Ensure you have Python installed along with the necessary dependencies.
2. Run the following command in the project root directory:
   ```bash
   python word_and_positional_embeddings.py
